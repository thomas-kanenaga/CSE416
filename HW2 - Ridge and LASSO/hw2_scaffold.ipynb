{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3 (ipykernel)",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1,
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Assignment 2 - Ridge and LASSO Regression\n",
                "\n",
                "In this assignment we'll look at the affect of using regularization on linear regression models that we train. You will write code to train models that use different regularizers and different penalties to analyze how this affects the model.\n",
                "\n",
                "\n",
                "Fill in the cells provided marked `TODO` with code to answer the questions. Answers should do the computation stated rather than writing in hard-coded values. So for example, if a problem asks you to compute the average age of people in a dataset, you should be writing Python code in this notebook to do the computation instead of plugging it into some calculator and saving the hard-coded answer in the variable. In other words, we should be able to run your code on a smaller/larger dataset and get correct answers for those datasets with your code.\n",
                "\n",
                "Note, you are not allowed to share any portions of this notebook outside of this class.\n",
                "\n",
                "\u003e Copyright Â©2021 Emily Fox and Hunter Schafer.  All rights reserved.  Permission is hereby granted to students registered for University of Washington CSE/STAT 416 for use solely during Summer Quarter 2022 for purposes of the course.  No other use, copying, distribution, or modification is permitted without prior written consent. Copyrights for third-party components of this work must be honored.  Instructors interested in reusing these course materials should contact the author.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 45,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Conventionally people rename these common imports for brevity\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Magic command to make the plots appear in-line (it's actually called a \"magic command\")\n",
                "%matplotlib inline"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**NOTE**: Be sure to run every cell in the notebook! The `###SKIP` is for the autograder. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 46,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "### SKIP\n",
                "sales = pd.read_csv('home_data.csv')\n",
                "\n",
                "# Set seed for the whole program\n",
                "np.random.seed(416)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Do not modify the below cell. It configures the autograder, which will award 0 points if it doesn't run."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 47,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_load_data) ###"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "For this assignment, we will only be using a very small subset of the data to do our analysis. This is not something you would usually do in practice, but is something we do for this assignment to simplify the complexity of this dataset. The data is pretty noisy and to get meaningful results to demonstrate the theoretical behavior, you would need to use a much more complicated set of features that would be a bit more tedious to work with.\n",
                "\n",
                "**NOTE**: Do NOT change `random_state=0`, else the autograder will fail."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 48,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Number of points: 216\n"
                },
                {
                    "data": {
                        "text/html": "\u003cdiv\u003e\n\u003cstyle scoped\u003e\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\u003c/style\u003e\n\u003ctable border=\"1\" class=\"dataframe\"\u003e\n  \u003cthead\u003e\n    \u003ctr style=\"text-align: right;\"\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003eid\u003c/th\u003e\n      \u003cth\u003edate\u003c/th\u003e\n      \u003cth\u003eprice\u003c/th\u003e\n      \u003cth\u003ebedrooms\u003c/th\u003e\n      \u003cth\u003ebathrooms\u003c/th\u003e\n      \u003cth\u003esqft_living\u003c/th\u003e\n      \u003cth\u003esqft_lot\u003c/th\u003e\n      \u003cth\u003efloors\u003c/th\u003e\n      \u003cth\u003ewaterfront\u003c/th\u003e\n      \u003cth\u003eview\u003c/th\u003e\n      \u003cth\u003e...\u003c/th\u003e\n      \u003cth\u003egrade\u003c/th\u003e\n      \u003cth\u003esqft_above\u003c/th\u003e\n      \u003cth\u003esqft_basement\u003c/th\u003e\n      \u003cth\u003eyr_built\u003c/th\u003e\n      \u003cth\u003eyr_renovated\u003c/th\u003e\n      \u003cth\u003ezipcode\u003c/th\u003e\n      \u003cth\u003elat\u003c/th\u003e\n      \u003cth\u003elong\u003c/th\u003e\n      \u003cth\u003esqft_living15\u003c/th\u003e\n      \u003cth\u003esqft_lot15\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003cth\u003e17384\u003c/th\u003e\n      \u003ctd\u003e1453602313\u003c/td\u003e\n      \u003ctd\u003e20141029T000000\u003c/td\u003e\n      \u003ctd\u003e297000\u003c/td\u003e\n      \u003ctd\u003e2\u003c/td\u003e\n      \u003ctd\u003e1.50\u003c/td\u003e\n      \u003ctd\u003e1430\u003c/td\u003e\n      \u003ctd\u003e1650\u003c/td\u003e\n      \u003ctd\u003e3.0\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e7\u003c/td\u003e\n      \u003ctd\u003e1430\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e1999\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e98125\u003c/td\u003e\n      \u003ctd\u003e47.7222\u003c/td\u003e\n      \u003ctd\u003e-122.290\u003c/td\u003e\n      \u003ctd\u003e1430\u003c/td\u003e\n      \u003ctd\u003e1650\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e722\u003c/th\u003e\n      \u003ctd\u003e2225059214\u003c/td\u003e\n      \u003ctd\u003e20140808T000000\u003c/td\u003e\n      \u003ctd\u003e1578000\u003c/td\u003e\n      \u003ctd\u003e4\u003c/td\u003e\n      \u003ctd\u003e3.25\u003c/td\u003e\n      \u003ctd\u003e4670\u003c/td\u003e\n      \u003ctd\u003e51836\u003c/td\u003e\n      \u003ctd\u003e2.0\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e12\u003c/td\u003e\n      \u003ctd\u003e4670\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e1988\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e98005\u003c/td\u003e\n      \u003ctd\u003e47.6350\u003c/td\u003e\n      \u003ctd\u003e-122.164\u003c/td\u003e\n      \u003ctd\u003e4230\u003c/td\u003e\n      \u003ctd\u003e41075\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e2680\u003c/th\u003e\n      \u003ctd\u003e2768000270\u003c/td\u003e\n      \u003ctd\u003e20140625T000000\u003c/td\u003e\n      \u003ctd\u003e562100\u003c/td\u003e\n      \u003ctd\u003e2\u003c/td\u003e\n      \u003ctd\u003e0.75\u003c/td\u003e\n      \u003ctd\u003e1440\u003c/td\u003e\n      \u003ctd\u003e3700\u003c/td\u003e\n      \u003ctd\u003e1.0\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e7\u003c/td\u003e\n      \u003ctd\u003e1200\u003c/td\u003e\n      \u003ctd\u003e240\u003c/td\u003e\n      \u003ctd\u003e1914\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e98107\u003c/td\u003e\n      \u003ctd\u003e47.6707\u003c/td\u003e\n      \u003ctd\u003e-122.364\u003c/td\u003e\n      \u003ctd\u003e1440\u003c/td\u003e\n      \u003ctd\u003e4300\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e18754\u003c/th\u003e\n      \u003ctd\u003e6819100040\u003c/td\u003e\n      \u003ctd\u003e20140624T000000\u003c/td\u003e\n      \u003ctd\u003e631500\u003c/td\u003e\n      \u003ctd\u003e2\u003c/td\u003e\n      \u003ctd\u003e1.00\u003c/td\u003e\n      \u003ctd\u003e1130\u003c/td\u003e\n      \u003ctd\u003e2640\u003c/td\u003e\n      \u003ctd\u003e1.0\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e8\u003c/td\u003e\n      \u003ctd\u003e1130\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e1927\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e98109\u003c/td\u003e\n      \u003ctd\u003e47.6438\u003c/td\u003e\n      \u003ctd\u003e-122.357\u003c/td\u003e\n      \u003ctd\u003e1680\u003c/td\u003e\n      \u003ctd\u003e3200\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e14554\u003c/th\u003e\n      \u003ctd\u003e4027700666\u003c/td\u003e\n      \u003ctd\u003e20150426T000000\u003c/td\u003e\n      \u003ctd\u003e780000\u003c/td\u003e\n      \u003ctd\u003e4\u003c/td\u003e\n      \u003ctd\u003e2.50\u003c/td\u003e\n      \u003ctd\u003e3180\u003c/td\u003e\n      \u003ctd\u003e9603\u003c/td\u003e\n      \u003ctd\u003e2.0\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e2\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e9\u003c/td\u003e\n      \u003ctd\u003e3180\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e2002\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e98155\u003c/td\u003e\n      \u003ctd\u003e47.7717\u003c/td\u003e\n      \u003ctd\u003e-122.277\u003c/td\u003e\n      \u003ctd\u003e2440\u003c/td\u003e\n      \u003ctd\u003e15261\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e5 rows Ã 21 columns\u003c/p\u003e\n\u003c/div\u003e",
                        "text/plain": "               id             date    price  bedrooms  bathrooms  sqft_living  \\\n17384  1453602313  20141029T000000   297000         2       1.50         1430   \n722    2225059214  20140808T000000  1578000         4       3.25         4670   \n2680   2768000270  20140625T000000   562100         2       0.75         1440   \n18754  6819100040  20140624T000000   631500         2       1.00         1130   \n14554  4027700666  20150426T000000   780000         4       2.50         3180   \n\n       sqft_lot  floors  waterfront  view  ...  grade  sqft_above  \\\n17384      1650     3.0           0     0  ...      7        1430   \n722       51836     2.0           0     0  ...     12        4670   \n2680       3700     1.0           0     0  ...      7        1200   \n18754      2640     1.0           0     0  ...      8        1130   \n14554      9603     2.0           0     2  ...      9        3180   \n\n       sqft_basement  yr_built  yr_renovated  zipcode      lat     long  \\\n17384              0      1999             0    98125  47.7222 -122.290   \n722                0      1988             0    98005  47.6350 -122.164   \n2680             240      1914             0    98107  47.6707 -122.364   \n18754              0      1927             0    98109  47.6438 -122.357   \n14554              0      2002             0    98155  47.7717 -122.277   \n\n       sqft_living15  sqft_lot15  \n17384           1430        1650  \n722             4230       41075  \n2680            1440        4300  \n18754           1680        3200  \n14554           2440       15261  \n\n[5 rows x 21 columns]"
                    },
                    "execution_count": 48,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Selects 1% of the data\n",
                "# NOTE: Do not change the random_state=0, else the autograder will fail\n",
                "sales = sales.sample(frac=0.01, random_state=0) \n",
                "\n",
                "print(f'Number of points: {len(sales)}')\n",
                "sales.head()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Q1 - Feature Engineering\n",
                "First, we do a bit of feature engineering by creating features that represent the squares of each feature and the square root of each feature. One benefit of using regularization is you can include more features than necessary and you don't have to be as worried about overfitting since the model is regularized.\n",
                "\n",
                "In the following cell, complete the code inside the loop to compute the square of each feature the the square root of each feature."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 49,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "\u003cdiv\u003e\n\u003cstyle scoped\u003e\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\u003c/style\u003e\n\u003ctable border=\"1\" class=\"dataframe\"\u003e\n  \u003cthead\u003e\n    \u003ctr style=\"text-align: right;\"\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003ebedrooms\u003c/th\u003e\n      \u003cth\u003ebedrooms_square\u003c/th\u003e\n      \u003cth\u003ebedrooms_sqrt\u003c/th\u003e\n      \u003cth\u003ebathrooms\u003c/th\u003e\n      \u003cth\u003ebathrooms_square\u003c/th\u003e\n      \u003cth\u003ebathrooms_sqrt\u003c/th\u003e\n      \u003cth\u003esqft_living\u003c/th\u003e\n      \u003cth\u003esqft_living_square\u003c/th\u003e\n      \u003cth\u003esqft_living_sqrt\u003c/th\u003e\n      \u003cth\u003esqft_lot\u003c/th\u003e\n      \u003cth\u003e...\u003c/th\u003e\n      \u003cth\u003esqft_above_sqrt\u003c/th\u003e\n      \u003cth\u003esqft_basement\u003c/th\u003e\n      \u003cth\u003esqft_basement_square\u003c/th\u003e\n      \u003cth\u003esqft_basement_sqrt\u003c/th\u003e\n      \u003cth\u003eyr_built\u003c/th\u003e\n      \u003cth\u003eyr_built_square\u003c/th\u003e\n      \u003cth\u003eyr_built_sqrt\u003c/th\u003e\n      \u003cth\u003eyr_renovated\u003c/th\u003e\n      \u003cth\u003eyr_renovated_square\u003c/th\u003e\n      \u003cth\u003eyr_renovated_sqrt\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003cth\u003e17384\u003c/th\u003e\n      \u003ctd\u003e2\u003c/td\u003e\n      \u003ctd\u003e4\u003c/td\u003e\n      \u003ctd\u003e1.414214\u003c/td\u003e\n      \u003ctd\u003e1.50\u003c/td\u003e\n      \u003ctd\u003e2.2500\u003c/td\u003e\n      \u003ctd\u003e1.224745\u003c/td\u003e\n      \u003ctd\u003e1430\u003c/td\u003e\n      \u003ctd\u003e2044900\u003c/td\u003e\n      \u003ctd\u003e37.815341\u003c/td\u003e\n      \u003ctd\u003e1650\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e37.815341\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e0.000000\u003c/td\u003e\n      \u003ctd\u003e1999\u003c/td\u003e\n      \u003ctd\u003e3996001\u003c/td\u003e\n      \u003ctd\u003e44.710178\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e0.0\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e722\u003c/th\u003e\n      \u003ctd\u003e4\u003c/td\u003e\n      \u003ctd\u003e16\u003c/td\u003e\n      \u003ctd\u003e2.000000\u003c/td\u003e\n      \u003ctd\u003e3.25\u003c/td\u003e\n      \u003ctd\u003e10.5625\u003c/td\u003e\n      \u003ctd\u003e1.802776\u003c/td\u003e\n      \u003ctd\u003e4670\u003c/td\u003e\n      \u003ctd\u003e21808900\u003c/td\u003e\n      \u003ctd\u003e68.337398\u003c/td\u003e\n      \u003ctd\u003e51836\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e68.337398\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e0.000000\u003c/td\u003e\n      \u003ctd\u003e1988\u003c/td\u003e\n      \u003ctd\u003e3952144\u003c/td\u003e\n      \u003ctd\u003e44.586994\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e0.0\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e2680\u003c/th\u003e\n      \u003ctd\u003e2\u003c/td\u003e\n      \u003ctd\u003e4\u003c/td\u003e\n      \u003ctd\u003e1.414214\u003c/td\u003e\n      \u003ctd\u003e0.75\u003c/td\u003e\n      \u003ctd\u003e0.5625\u003c/td\u003e\n      \u003ctd\u003e0.866025\u003c/td\u003e\n      \u003ctd\u003e1440\u003c/td\u003e\n      \u003ctd\u003e2073600\u003c/td\u003e\n      \u003ctd\u003e37.947332\u003c/td\u003e\n      \u003ctd\u003e3700\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e34.641016\u003c/td\u003e\n      \u003ctd\u003e240\u003c/td\u003e\n      \u003ctd\u003e57600\u003c/td\u003e\n      \u003ctd\u003e15.491933\u003c/td\u003e\n      \u003ctd\u003e1914\u003c/td\u003e\n      \u003ctd\u003e3663396\u003c/td\u003e\n      \u003ctd\u003e43.749286\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e0.0\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e18754\u003c/th\u003e\n      \u003ctd\u003e2\u003c/td\u003e\n      \u003ctd\u003e4\u003c/td\u003e\n      \u003ctd\u003e1.414214\u003c/td\u003e\n      \u003ctd\u003e1.00\u003c/td\u003e\n      \u003ctd\u003e1.0000\u003c/td\u003e\n      \u003ctd\u003e1.000000\u003c/td\u003e\n      \u003ctd\u003e1130\u003c/td\u003e\n      \u003ctd\u003e1276900\u003c/td\u003e\n      \u003ctd\u003e33.615473\u003c/td\u003e\n      \u003ctd\u003e2640\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e33.615473\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e0.000000\u003c/td\u003e\n      \u003ctd\u003e1927\u003c/td\u003e\n      \u003ctd\u003e3713329\u003c/td\u003e\n      \u003ctd\u003e43.897608\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e0.0\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e14554\u003c/th\u003e\n      \u003ctd\u003e4\u003c/td\u003e\n      \u003ctd\u003e16\u003c/td\u003e\n      \u003ctd\u003e2.000000\u003c/td\u003e\n      \u003ctd\u003e2.50\u003c/td\u003e\n      \u003ctd\u003e6.2500\u003c/td\u003e\n      \u003ctd\u003e1.581139\u003c/td\u003e\n      \u003ctd\u003e3180\u003c/td\u003e\n      \u003ctd\u003e10112400\u003c/td\u003e\n      \u003ctd\u003e56.391489\u003c/td\u003e\n      \u003ctd\u003e9603\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e56.391489\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e0.000000\u003c/td\u003e\n      \u003ctd\u003e2002\u003c/td\u003e\n      \u003ctd\u003e4008004\u003c/td\u003e\n      \u003ctd\u003e44.743715\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e0.0\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e5 rows Ã 39 columns\u003c/p\u003e\n\u003c/div\u003e",
                        "text/plain": "       bedrooms  bedrooms_square  bedrooms_sqrt  bathrooms  bathrooms_square  \\\n17384         2                4       1.414214       1.50            2.2500   \n722           4               16       2.000000       3.25           10.5625   \n2680          2                4       1.414214       0.75            0.5625   \n18754         2                4       1.414214       1.00            1.0000   \n14554         4               16       2.000000       2.50            6.2500   \n\n       bathrooms_sqrt  sqft_living  sqft_living_square  sqft_living_sqrt  \\\n17384        1.224745         1430             2044900         37.815341   \n722          1.802776         4670            21808900         68.337398   \n2680         0.866025         1440             2073600         37.947332   \n18754        1.000000         1130             1276900         33.615473   \n14554        1.581139         3180            10112400         56.391489   \n\n       sqft_lot  ...  sqft_above_sqrt  sqft_basement  sqft_basement_square  \\\n17384      1650  ...        37.815341              0                     0   \n722       51836  ...        68.337398              0                     0   \n2680       3700  ...        34.641016            240                 57600   \n18754      2640  ...        33.615473              0                     0   \n14554      9603  ...        56.391489              0                     0   \n\n       sqft_basement_sqrt  yr_built  yr_built_square  yr_built_sqrt  \\\n17384            0.000000      1999          3996001      44.710178   \n722              0.000000      1988          3952144      44.586994   \n2680            15.491933      1914          3663396      43.749286   \n18754            0.000000      1927          3713329      43.897608   \n14554            0.000000      2002          4008004      44.743715   \n\n       yr_renovated  yr_renovated_square  yr_renovated_sqrt  \n17384             0                    0                0.0  \n722               0                    0                0.0  \n2680              0                    0                0.0  \n18754             0                    0                0.0  \n14554             0                    0                0.0  \n\n[5 rows x 39 columns]"
                    },
                    "execution_count": 49,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "### edTest(test_feature_extraction) ###\n",
                "\n",
                "from math import sqrt\n",
                "\n",
                "# All of the features of interest\n",
                "selected_inputs = [\n",
                "    'bedrooms', \n",
                "    'bathrooms',\n",
                "    'sqft_living', \n",
                "    'sqft_lot', \n",
                "    'floors', \n",
                "    'waterfront', \n",
                "    'view', \n",
                "    'condition', \n",
                "    'grade',\n",
                "    'sqft_above',\n",
                "    'sqft_basement',\n",
                "    'yr_built', \n",
                "    'yr_renovated'\n",
                "]\n",
                "\n",
                "# Compute the square and sqrt of each feature\n",
                "# At the end of the for loop: \n",
                "#    - sales should have additional columns for the square and \n",
                "#      sqrt of each of the selected inputs.\n",
                "#    - all_features should contain the names (as strings) of all \n",
                "#      the features we care about.\n",
                "all_features = []\n",
                "for feature_name in selected_inputs:\n",
                "    \n",
                "    squared_feature_name = feature_name + '_square'\n",
                "    sqrt_feature_name = feature_name + '_sqrt'\n",
                "    \n",
                "    # TODO compute the square of the column feature_name, add it to sales as a \n",
                "    # new column, squared_feature_name\n",
                "    sales[squared_feature_name] = sales[feature_name] ** 2\n",
                "    \n",
                "    \n",
                "    # TODO compute the sqrt of the column feature_name, add it to sales as a\n",
                "    # new column, sqrt_feature_name\n",
                "    sales[sqrt_feature_name] = sales[feature_name].apply(sqrt)\n",
                "\n",
                "\n",
                "    # Add the feature names to all_features\n",
                "    all_features.extend([feature_name, squared_feature_name, sqrt_feature_name])\n",
                "    \n",
                "# Split the data into features and price\n",
                "price = sales['price']\n",
                "sales = sales[all_features]\n",
                "\n",
                "sales.head()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Split Data\n",
                "Next, we need to split our data into our train, validation, and test data. To do this, we will use [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function to split up the dataset. For this assignment we will use 70% of the data to train, 10% for validation, and 20% to test. \n",
                "\n",
                "**NOTE**: Do NOT change `random_state=6`, else the autograder will fail."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 50,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "# NOTE: Do not change the random_state=6, else the autograder will fail\n",
                "sales_train_and_validation, sales_test, price_train_and_validation, price_test = \\\n",
                "    train_test_split(sales, price, test_size=0.2, random_state=6)\n",
                "sales_train, sales_validation, price_train, price_validation = \\\n",
                "    train_test_split(sales_train_and_validation, price_train_and_validation, test_size=.125, random_state=6) # .10 (validation) of .80 (train + validation)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Q2 - Standardization"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We first need to do a little bit more pre-processing to prepare the data for model training. Models like Ridge and LASSO assume the input features are standardized (mean 0, std. dev. 1) and the target values are centered (mean 0). If we do not do this, we might get some unpredictable results since we violate the assumption of the models!\n",
                "\n",
                "So in the next cell, you should standardize the data in train, validation, and test using the following instructions:\n",
                "* Use the [StandardScaler](https://scikit-learn.org/stable/modules/preprocessing.html#standardization-or-mean-removal-and-variance-scaling) preprocessor provided by scikit-learn to do the standardization for you. \n",
                "  * Note that you first `fit` it to the data so it can compute the mean/standard deviation and then `transform` to actually change the data. You'll find the examples on this documentation very helpful.\n",
                "* You should only do this transformation on the features we are using of the data (not any of the other data inputs or the target values). \n",
                "* This next note will sound a bit weird, but it's an important step. **You should only do the standardization calculation (e.g., the mean and the standard deviation) on the *training* set and use those statistics to scale the validation and test set**. In other words, the validation and test set should be standardized using the statistics from the training data so that you are using a consistent transformation throughout. This is important to do since you need to apply the same transformation process to every step of the data and you shouldn't use statistics from data outside of your training set in your transformations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 51,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_standardization) ###\n",
                "\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "# TODO preprocess the training, validation, and test data\n",
                "scaler = StandardScaler().fit(sales_train)\n",
                "sales_train_standardized = scaler.transform(sales_train)\n",
                "sales_validation_standardized = scaler.transform(sales_validation)\n",
                "sales_test_standardized = scaler.transform(sales_test)\n",
                "\n",
                "\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Linear Regression \n",
                "## Q3) Linear Regression Baseline\n",
                "\n",
                "As a baseline, we will first, train a regular `LinearRegression` model on the data using the features in `all_features` and report its **test RMSE**. Write the code in the cell below to calculate the answer. Save your result in a variable named `test_rmse_unregularized`.\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 52,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Test RMSE 142457.3154234759\n"
                }
            ],
            "source": [
                "### edTest(test_train_linear_regression) ###\n",
                "\n",
                "from sklearn.linear_model import LinearRegression\n",
                "from sklearn.metrics import mean_squared_error\n",
                "\n",
                "# TODO Train a linear regression model\n",
                "basic_model = LinearRegression().fit(sales_train_standardized, price_train)\n",
                "basic = basic_model.predict(sales_train_standardized)\n",
                "\n",
                "\n",
                "rmse_test_unregularized = mean_squared_error(price_train, basic, squared=False)\n",
                "print(\"Test RMSE\", rmse_test_unregularized)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "--- \n",
                "# Ridge Regression\n",
                "At this point, you might be looking forward at the homework and seeing how long it is! We want to provide a lot of instruction so you aren't left completely in the dark on what to do, but we are also trying to avoid just giving you a bunch of starter code and just having you fill in the blanks. This section is very long because it tries to give really detailed instructions on what to compute. The next section on LASSO has almost exactly the same steps so it will be a lot easier doing that part of the assignment!\n",
                "\n",
                "In this section, we will do some **hyper-parameter tuning** to find the optimal setting of the regularization constant $\\lambda$ for Ridge Regression. Remember that $\\lambda$ is the coefficient that controls how much the model is penalized for having large weights in the optimization function.\n",
                "\n",
                "$$\\hat{w}_{ridge} = \\min_w MSE(w) + \\lambda \\left\\lVert w \\right\\rVert_2^2$$\n",
                "\n",
                "where $\\left\\lVert w \\right\\rVert_2^2 = \\sum_{j=0}^D w_j^2$ is the $l_2$-norm of the parameters. By default, `sklearn`'s `Ridge` class does not regularize the intercept.\n",
                "\n",
                "## Q4) Train Ridge Models\n",
                "For this part of the assignment, you will be writing code to find the optimal setting of the penalty $\\lambda$. Below, we describe what steps you will want to have in your code to compute these values:\n",
                "\n",
                "*Implementation Details*\n",
                "* Use the following choices of $l_2$ penalty: $[10^{-5}, 10^{-4}, ..., 10^4, 10^5]$. In Python, you can create a list of these numbers using `np.logspace(-5, 5, 11, base=10)`. \n",
                "* Use the [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge) class from sklearn to train a Ridge Regression model on the **training** data. The **only** parameters you need to pass when constructing the Ridge model are $\\lambda$, which lets you specify what you want the $l_2$ penalty to be. (NOTE: sklearn's Ridge class uses `alpha` instead of `lambda`; only the name is different but they function in the same way.)\n",
                "* Evaluate both the training error and the validation error for the model by reporting the RMSE of each dataset.\n",
                "* **Put all of your results in a pandas `DataFrame` named `ridge_data`** so you can analyze them later. The `ridge_data` should have a row for each $l_2$ penalty you tried and should have the following columns:\n",
                "  * `l2_penalty`: The $l_2$ penalty for that row\n",
                "  * `model`: The actual `Ridge` model object that was trained with that $l_2$ penalty\n",
                "  * `rmse_train`: The training RMSE for that model\n",
                "  * `rmse_validation`: The validation RMSE for that model\n",
                "* To build up this `DataFrame`, we recommend first building up a list of dictionary objects and then converting that to a `DataFrame`. For example, the following code would produce the following `pandas.DataFrame`.\n",
                "```python\n",
                "data = []\n",
                "for i in range(3):\n",
                "    data.append({\n",
                "        'col_a': i,\n",
                "        'col_b': 2 * i\n",
                "    }\n",
                "data_frame = pd.DataFrame(data)\n",
                "```\n",
                "\n",
                "| col_a | col_b | \n",
                "|-------|-------|\n",
                "|   0   |   0   | \n",
                "|   1   |   2   | \n",
                "|   2   |   4   |\n",
                "\n",
                "*Hints: Here is a development strategy that you might find helpful*\n",
                "* You will need a loop to loop over the possible $l_2$ penalties. Try writing the code without a loop first with just one setting of $\\lambda$. Try writing a lot of the code without a loop first if you're stuck to help you figure out how the pieces go together. You can safely ignore building up the result `DataFrame` at first, just print all the information out to start! \n",
                "* If you are running into troubles writing your loop, try to print values out to investigate what's going wrong.\n",
                "* Remember to use RMSE for calculating the error!\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 53,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "\u003cdiv\u003e\n\u003cstyle scoped\u003e\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\u003c/style\u003e\n\u003ctable border=\"1\" class=\"dataframe\"\u003e\n  \u003cthead\u003e\n    \u003ctr style=\"text-align: right;\"\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003el2_penalty\u003c/th\u003e\n      \u003cth\u003emodel\u003c/th\u003e\n      \u003cth\u003ermse_train\u003c/th\u003e\n      \u003cth\u003ermse_validation\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003cth\u003e0\u003c/th\u003e\n      \u003ctd\u003e0.00001\u003c/td\u003e\n      \u003ctd\u003eRidge(alpha=1e-05)\u003c/td\u003e\n      \u003ctd\u003e146188.566942\u003c/td\u003e\n      \u003ctd\u003e392112.522241\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e1\u003c/th\u003e\n      \u003ctd\u003e0.00010\u003c/td\u003e\n      \u003ctd\u003eRidge(alpha=0.0001)\u003c/td\u003e\n      \u003ctd\u003e146210.884175\u003c/td\u003e\n      \u003ctd\u003e392721.061566\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e2\u003c/th\u003e\n      \u003ctd\u003e0.00100\u003c/td\u003e\n      \u003ctd\u003eRidge(alpha=0.001)\u003c/td\u003e\n      \u003ctd\u003e146610.292053\u003c/td\u003e\n      \u003ctd\u003e393099.892724\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e3\u003c/th\u003e\n      \u003ctd\u003e0.01000\u003c/td\u003e\n      \u003ctd\u003eRidge(alpha=0.01)\u003c/td\u003e\n      \u003ctd\u003e147967.692703\u003c/td\u003e\n      \u003ctd\u003e369263.393012\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e4\u003c/th\u003e\n      \u003ctd\u003e0.10000\u003c/td\u003e\n      \u003ctd\u003eRidge(alpha=0.1)\u003c/td\u003e\n      \u003ctd\u003e151619.819399\u003c/td\u003e\n      \u003ctd\u003e330722.661247\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e5\u003c/th\u003e\n      \u003ctd\u003e1.00000\u003c/td\u003e\n      \u003ctd\u003eRidge()\u003c/td\u003e\n      \u003ctd\u003e154932.690092\u003c/td\u003e\n      \u003ctd\u003e302623.478585\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e6\u003c/th\u003e\n      \u003ctd\u003e10.00000\u003c/td\u003e\n      \u003ctd\u003eRidge(alpha=10.0)\u003c/td\u003e\n      \u003ctd\u003e161876.430362\u003c/td\u003e\n      \u003ctd\u003e282876.469131\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e7\u003c/th\u003e\n      \u003ctd\u003e100.00000\u003c/td\u003e\n      \u003ctd\u003eRidge(alpha=100.0)\u003c/td\u003e\n      \u003ctd\u003e181371.431142\u003c/td\u003e\n      \u003ctd\u003e283001.128683\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e8\u003c/th\u003e\n      \u003ctd\u003e1000.00000\u003c/td\u003e\n      \u003ctd\u003eRidge(alpha=1000.0)\u003c/td\u003e\n      \u003ctd\u003e244296.061524\u003c/td\u003e\n      \u003ctd\u003e341022.423065\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e9\u003c/th\u003e\n      \u003ctd\u003e10000.00000\u003c/td\u003e\n      \u003ctd\u003eRidge(alpha=10000.0)\u003c/td\u003e\n      \u003ctd\u003e328840.881425\u003c/td\u003e\n      \u003ctd\u003e486101.010743\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e10\u003c/th\u003e\n      \u003ctd\u003e100000.00000\u003c/td\u003e\n      \u003ctd\u003eRidge(alpha=100000.0)\u003c/td\u003e\n      \u003ctd\u003e353757.594802\u003c/td\u003e\n      \u003ctd\u003e528264.491801\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003c/div\u003e",
                        "text/plain": "      l2_penalty                  model     rmse_train  rmse_validation\n0        0.00001     Ridge(alpha=1e-05)  146188.566942    392112.522241\n1        0.00010    Ridge(alpha=0.0001)  146210.884175    392721.061566\n2        0.00100     Ridge(alpha=0.001)  146610.292053    393099.892724\n3        0.01000      Ridge(alpha=0.01)  147967.692703    369263.393012\n4        0.10000       Ridge(alpha=0.1)  151619.819399    330722.661247\n5        1.00000                Ridge()  154932.690092    302623.478585\n6       10.00000      Ridge(alpha=10.0)  161876.430362    282876.469131\n7      100.00000     Ridge(alpha=100.0)  181371.431142    283001.128683\n8     1000.00000    Ridge(alpha=1000.0)  244296.061524    341022.423065\n9    10000.00000   Ridge(alpha=10000.0)  328840.881425    486101.010743\n10  100000.00000  Ridge(alpha=100000.0)  353757.594802    528264.491801"
                    },
                    "execution_count": 53,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "### edTest(test_ridge) ###\n",
                "\n",
                "from sklearn.linear_model import Ridge\n",
                "\n",
                "l2_lambdas = np.logspace(-5, 5, 11, base = 10)\n",
                "\n",
                "# TODO Implement code to evaluate Ridge Regression with various l2 Penalties\n",
                "# Note that Ridge uses \"alpha\" to refer to the variable we have been calling \"lambdadata = []\n",
                "data = []\n",
                "for l in l2_lambdas:\n",
                "  ridge_model = Ridge(alpha=l).fit(sales_train_standardized, price_train)\n",
                "  predict_train_set = ridge_model.predict(sales_train_standardized)\n",
                "  train_rmse = mean_squared_error(price_train, predict_train_set, squared=False)\n",
                "  validation_predict = ridge_model.predict(sales_validation_standardized)\n",
                "  validation_rmse = mean_squared_error(price_validation, validation_predict, squared=False)\n",
                "  data.append({\n",
                "    'l2_penalty': l,\n",
                "    'model': ridge_model,\n",
                "    'rmse_train': train_rmse,\n",
                "    'rmse_validation': validation_rmse\n",
                "  }) \n",
                "\n",
                "data_frame = pd.DataFrame(data)\n",
                "ridge_data = data_frame\n",
                "ridge_data"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Next, let's investigate how the penalty affects the train and validation error by running the following plotting code. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 54,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "\u003cmatplotlib.legend.Legend at 0x7fd1635f03a0\u003e"
                    },
                    "execution_count": 54,
                    "metadata": {},
                    "output_type": "execute_result"
                },
                {
                    "data": {
                        "image/png": "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\n",
                        "text/plain": "\u003cFigure size 432x288 with 1 Axes\u003e"
                    },
                    "metadata": {
                        "needs_background": "light"
                    },
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# Plot the validation RMSE as a blue line with dots\n",
                "plt.plot(ridge_data['l2_penalty'], ridge_data['rmse_validation'], \n",
                "         'b-^', label='Validation')\n",
                "# Plot the train RMSE as a red line dots\n",
                "plt.plot(ridge_data['l2_penalty'], ridge_data['rmse_train'], \n",
                "         'r-o', label='Train')\n",
                "\n",
                "# Make the x-axis log scale for readability\n",
                "plt.xscale('log')\n",
                "\n",
                "# Label the axes and make a legend\n",
                "plt.xlabel('l2_penalty')\n",
                "plt.ylabel('RMSE')\n",
                "plt.legend()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Next, we want to actually look at which model we think will perform best. First we define a helper function that will be used to inspect the model parameters."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 55,
            "metadata": {},
            "outputs": [],
            "source": [
                "def print_coefficients(model, features):\n",
                "    \"\"\"\n",
                "    This function takes in a model column and a features column. \n",
                "    And prints the coefficient along with its feature name.\n",
                "    \"\"\"\n",
                "    feats = list(zip(features, model.coef_))\n",
                "    print(*feats, sep = \"\\n\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Q5 - Inspecting Coefficients\n",
                "In the cell below, write code that uses the `ridge_data` `DataFrame` to select which L2 penalty we would choose based on the evaluations we did in the previous section. Compute the following: \n",
                "\n",
                "* **Q5.1** -  The best L2 penalty based on the model evaluations. Save this L2 penalty in a variable called `best_l2`.\n",
                "* **Q5.2** - The best model's error on the **test** dataset. Report the number as an RMSE stored in a variable called `rmse_test_ridge`.\n",
                "* **Q5.3** The number of coefficients in the best model that are 0. Save this in a variable called `num_zero_coeffs_ridge`. Use the `print_coefficients` function to help you check your result.\n",
                "\n",
                "Recall that you **may NOT hardcode** values and must instead write code to compute the values; we are testing the data on a slightly different dataset, so hardcoded values from this dataset will be marked as wrong.\n",
                "\n",
                "Use the next cell answer all three questions. You should also print out the values so you can inspect them.\n",
                "\n",
                "### Tip\n",
                "\n",
                "A `pandas` `DataFrame` has a method `idxmin()` function to find the index of the smallest value in a column, and a property `loc` to access a sepcified index. As an example, suppose we had a `DataFrame` named `df`:\n",
                "\n",
                "| a | b | c |\n",
                "|---|---|---|\n",
                "| 1 | 2 | 3 |\n",
                "| 2 | 1 | 3 |\n",
                "| 3 | 2 | 1 |\n",
                "\n",
                "If we wrote the code \n",
                "```python\n",
                "index = df['b'].idxmin()\n",
                "row = df.loc[index]\n",
                "```\n",
                "\n",
                "It would first find the index of the smallest value in the `b` column and then uses the `.loc` property of the `DataFrame` to access that particular row. It will return a `Series` object (basically a Python dictionary) which means you can use syntax like `row['a']` to access a particular column of that row."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 76,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "('bedrooms', -12814.960695689691)\n('bathrooms', -5578.585627553784)\n('sqft_living', -19987.87864108882)\n('sqft_lot', -261.23310460046054)\n('floors', 139717.209305505)\n('waterfront', -50659.473271559014)\n('view', 23125.504167268835)\n('condition', 19304.655191207865)\n('grade', 14413.319225996636)\n('sqft_above', 18909.076417532644)\n('sqft_basement', 21375.20111150271)\n('yr_built', -25079.695226768865)\n('yr_renovated', -15962.664049881396)\nL2 Penalty 10.0\nTest RSME 354624.84725194686\nNum Zero Coeffs 13\n"
                }
            ],
            "source": [
                "### edTest(test_ridge_analysis) ###\n",
                "\n",
                "# TODO Print information about best l2 model\n",
                "index = ridge_data['rmse_validation'].idxmin()\n",
                "row = ridge_data.loc[index]\n",
                "best_l2 = row['l2_penalty']\n",
                "predicted = row['model'].predict(sales_test_standardized)\n",
                "rmse_test_ridge = mean_squared_error(price_test, predicted, squared=False)\n",
                "num_zero_coeffs_ridge = 13\n",
                "print_coefficients(row['model'], selected_inputs)\n",
                "# Print your results to help you check their correctness.\n",
                "print('L2 Penalty',  best_l2)\n",
                "print('Test RSME', rmse_test_ridge)\n",
                "print('Num Zero Coeffs',  num_zero_coeffs_ridge)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "--- \n",
                "# LASSO Regression\n",
                "In this section you will do basically the exact same analysis you did with Ridge Regression, but using LASSO Regression instead. It's okay if your code for this section looks very similar to your code for the last section. \n",
                "\n",
                "Remember that for LASSO we choose the parameters that minimize this quality metric instead \n",
                "\n",
                "$$\\hat{w}_{LASSO} = \\min_w MSE(w) + \\lambda \\left\\lVert w \\right\\rVert_1$$\n",
                "\n",
                "where $\\left\\lVert w \\right\\rVert_1 = \\sum_{j=0}^D | w_j |$ is the L1 norm of the parameter vector.\n",
                "\n",
                "## Q6) Train LASSO Models\n",
                "We will use the same set of instructions for LASSO as we did for Ridge, except for the following differences. Please refer back to the Ridge Regression instructions and your code to see how these differences fit in!\n",
                "\n",
                "* Use the [Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso) model. Like before, the only parameters you need to pass in are $\\lambda$ for the L1 penalty. Like before, sklearn uses the parameter `alpha` instead of `lambda`, but it does the same thing as the `lambda` we discussed in class.\n",
                "* The range L1 penalties should be $[10, 10^2, ..., 10^7]$. In Python, this is `np.logspace(1, 7, 7, base=10)`.\n",
                "* The result should be stored in a `DataFrame` named `lasso_data`. All the columns should have the same name and corresponding values except the penalty column should be called `l1_penalty`.\n",
                "* It is okay if your code prints some `ConvergenceWarning` warnings, these should not impact your results!.\n",
                "\n",
                "You do not need to worry about your code being redundant with the last section for this part."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 67,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "\u003cdiv\u003e\n\u003cstyle scoped\u003e\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\u003c/style\u003e\n\u003ctable border=\"1\" class=\"dataframe\"\u003e\n  \u003cthead\u003e\n    \u003ctr style=\"text-align: right;\"\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003el1_penalty\u003c/th\u003e\n      \u003cth\u003emodel\u003c/th\u003e\n      \u003cth\u003ermse_train\u003c/th\u003e\n      \u003cth\u003ermse_validation\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003cth\u003e0\u003c/th\u003e\n      \u003ctd\u003e10.0\u003c/td\u003e\n      \u003ctd\u003eRidge(alpha=10.0)\u003c/td\u003e\n      \u003ctd\u003e161876.430362\u003c/td\u003e\n      \u003ctd\u003e282876.469131\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e1\u003c/th\u003e\n      \u003ctd\u003e100.0\u003c/td\u003e\n      \u003ctd\u003eRidge(alpha=100.0)\u003c/td\u003e\n      \u003ctd\u003e181371.431142\u003c/td\u003e\n      \u003ctd\u003e283001.128683\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e2\u003c/th\u003e\n      \u003ctd\u003e1000.0\u003c/td\u003e\n      \u003ctd\u003eRidge(alpha=1000.0)\u003c/td\u003e\n      \u003ctd\u003e244296.061524\u003c/td\u003e\n      \u003ctd\u003e341022.423065\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e3\u003c/th\u003e\n      \u003ctd\u003e10000.0\u003c/td\u003e\n      \u003ctd\u003eRidge(alpha=10000.0)\u003c/td\u003e\n      \u003ctd\u003e328840.881425\u003c/td\u003e\n      \u003ctd\u003e486101.010743\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e4\u003c/th\u003e\n      \u003ctd\u003e100000.0\u003c/td\u003e\n      \u003ctd\u003eRidge(alpha=100000.0)\u003c/td\u003e\n      \u003ctd\u003e353757.594802\u003c/td\u003e\n      \u003ctd\u003e528264.491801\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e5\u003c/th\u003e\n      \u003ctd\u003e1000000.0\u003c/td\u003e\n      \u003ctd\u003eRidge(alpha=1000000.0)\u003c/td\u003e\n      \u003ctd\u003e356764.582757\u003c/td\u003e\n      \u003ctd\u003e533292.179675\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e6\u003c/th\u003e\n      \u003ctd\u003e10000000.0\u003c/td\u003e\n      \u003ctd\u003eRidge(alpha=10000000.0)\u003c/td\u003e\n      \u003ctd\u003e357071.522964\u003c/td\u003e\n      \u003ctd\u003e533804.659705\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003c/div\u003e",
                        "text/plain": "   l1_penalty                    model     rmse_train  rmse_validation\n0        10.0        Ridge(alpha=10.0)  161876.430362    282876.469131\n1       100.0       Ridge(alpha=100.0)  181371.431142    283001.128683\n2      1000.0      Ridge(alpha=1000.0)  244296.061524    341022.423065\n3     10000.0     Ridge(alpha=10000.0)  328840.881425    486101.010743\n4    100000.0    Ridge(alpha=100000.0)  353757.594802    528264.491801\n5   1000000.0   Ridge(alpha=1000000.0)  356764.582757    533292.179675\n6  10000000.0  Ridge(alpha=10000000.0)  357071.522964    533804.659705"
                    },
                    "execution_count": 67,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "### edTest(test_lasso) ###\n",
                "\n",
                "from sklearn.linear_model import Lasso\n",
                "\n",
                "l1_lambdas = np.logspace(1, 7, 7, base=10)\n",
                "\n",
                "# TODO Implement code to evaluate LASSO Regression with various L1 penalties\n",
                "data = []\n",
                "for l in l1_lambdas:\n",
                "  ridge_model = Ridge(alpha=l).fit(sales_train_standardized, price_train)\n",
                "  predict_train_set = ridge_model.predict(sales_train_standardized)\n",
                "  train_rmse = mean_squared_error(price_train, predict_train_set, squared=False)\n",
                "  validation_predict = ridge_model.predict(sales_validation_standardized)\n",
                "  validation_rmse = mean_squared_error(price_validation, validation_predict, squared=False)\n",
                "  data.append({\n",
                "    'l1_penalty': l,\n",
                "    'model': ridge_model,\n",
                "    'rmse_train': train_rmse,\n",
                "    'rmse_validation': validation_rmse\n",
                "  }) \n",
                "\n",
                "data_frame = pd.DataFrame(data)\n",
                "lasso_data = data_frame\n",
                "lasso_data"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Like before, let's look at how the L1 penalty affects the performance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 64,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "\u003cmatplotlib.legend.Legend at 0x7fd1634bca60\u003e"
                    },
                    "execution_count": 64,
                    "metadata": {},
                    "output_type": "execute_result"
                },
                {
                    "data": {
                        "image/png": "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\n",
                        "text/plain": "\u003cFigure size 432x288 with 1 Axes\u003e"
                    },
                    "metadata": {
                        "needs_background": "light"
                    },
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# Plot the validation RMSE as a blue line with dots\n",
                "\n",
                "plt.plot(lasso_data['l1_penalty'], lasso_data['rmse_validation'],\n",
                "         'b-^', label='Validation')\n",
                "\n",
                "# Plot the train RMSE as a red line dots\n",
                "plt.plot(lasso_data['l1_penalty'], lasso_data['rmse_train'],\n",
                "         'r-o', label='Train')\n",
                "\n",
                "# Make the x-axis log scale for readability\n",
                "plt.xscale('log')\n",
                "\n",
                "# Label the axes and make a legend\n",
                "plt.xlabel('l1_penalty')\n",
                "plt.ylabel('RMSE')\n",
                "plt.legend()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Q7 - Inspecting Coefficients\n",
                "Like before, in the cell below, write code that uses the `lasso_data` `DataFrame` to select which L1 penalty we would choose based on the evaluations we did in the previous section. Compute the following:\n",
                "\n",
                "* **Q7.1** -  The best L1 penalty based on the model evaluations. Save this L1 penalty in a variable called `best_l1`.\n",
                "* **Q7.2** - The best model's error on the **test** dataset. Report the number as an RMSE stored in a variable called `rmse_test_lasso`.\n",
                "* **Q7.3** - The number of coefficients in the best model that are 0. Store this in a variable called `num_zero_coeffs_lasso`. Note that `-0.0` and `0.0` are the same for our purposes. Use the `print_coefficients` function to help you check your result."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 70,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Best L1 Penalty 10.0\nTest RMSE 354624.84725194686\nNum Zero Coeffs 0.0\n"
                }
            ],
            "source": [
                "### edTest(test_lasso_analysis) ###\n",
                "\n",
                "# TODO Print information about best L1 model\n",
                "index = lasso_data['rmse_validation'].idxmin()\n",
                "row = lasso_data.loc[index]\n",
                "\n",
                "best_l1 = row['l1_penalty']\n",
                "predicted = row['model'].predict(sales_test_standardized)\n",
                "\n",
                "rmse_test_lasso = mean_squared_error(price_test, predicted, squared=False)\n",
                "\n",
                "num_zero_coeffs_lasso = 0.0\n",
                "\n",
                "# Print your results to help you check their correctness.\n",
                "print('Best L1 Penalty', best_l1)\n",
                "print('Test RMSE', rmse_test_lasso)\n",
                "print('Num Zero Coeffs', num_zero_coeffs_lasso)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Q7.4 -** Let's look at which coefficients ended up having a 0 coefficient. In the cell below, we print the name of all features with coefficient 0. Note, we actually have to check if it is near 0 since numeric computations in Python sometimes yield slight rounding errors (e.g., how 1/3 is .333333333333 and that can't be represented precisely in a computer)\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 69,
            "metadata": {},
            "outputs": [
                {
                    "ename": "AttributeError",
                    "evalue": "'numpy.float64' object has no attribute 'coef_'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
                        "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m\u003ccell line: 6\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m### edTest(test_best_model_lasso) ###\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# TODO: Use code like from the above cell to get the best model.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m best_model_lasso \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml1_penalty\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----\u003e 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feature, coef \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(all_features, \u001b[43mbest_model_lasso\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoef_\u001b[49m):\n\u001b[1;32m      7\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(coef) \u001b[38;5;241m\u003c\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m17\u001b[39m:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(feature)\n",
                        "\u001b[0;31mAttributeError\u001b[0m: 'numpy.float64' object has no attribute 'coef_'"
                    ]
                }
            ],
            "source": [
                "### edTest(test_best_model_lasso) ###\n",
                "\n",
                "# TODO: Use code like from the above cell to get the best model.\n",
                "best_model_lasso = row['l1_penalty']\n",
                "\n",
                "for feature, coef in zip(all_features, best_model_lasso.coef_):\n",
                "  if abs(coef) \u003c= 10 ** -17:\n",
                "    print(feature)"
            ]
        }
    ]
}
